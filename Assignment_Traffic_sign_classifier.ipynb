{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Traffic Sign Classifier\n",
    "\n",
    "---\n",
    "\n",
    "Using computer vision, deep learning, and convolutional neural networks, create a pipeline that classifies traffic signs.  Train and validate a model so it can classify traffic sign images using the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). After the model is trained, test it on images of traffic signs that you find on the web.\n",
    "\n",
    "The goals of this project are the following:\n",
    "\n",
    "- Load the data set\n",
    "- Explore, summarize and visualize the data set\n",
    "- Design, train and test a model architecture\n",
    "- Use the model to make predictions on new images\n",
    "- Use data augmentation and transfer learning\n",
    "- Analyze the softmax probabilities of the new images\n",
    "- Summarize the results of each step with a written report\n",
    "\n",
    "The writeup should be included in additional cells within the notebook. Some sample cells are included so that you have a guide of what is expected of the writeup. Additionally, add a README file with an introduction to what you developed, as well as the steps needed to run it.\n",
    "\n",
    "**Notes:** \n",
    "- Unless you have set up a GPU on a desktop with all the needed packages, train the model using a GPU enabled google colab environment.\n",
    "- Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run.\n",
    "\n",
    "**Rubric:**\n",
    "\n",
    "1. Uploaded the notbook with all cells executed and the writeup cells, and the HTML output of the code.\n",
    "2. Downloaded and loaded the dataset. There was a description of the data, and a visualization of one image of each category.\n",
    "3. Preprocessed the data, and provided a description of the preprocessing steps taken. Use data augmentation to improve the available data.\n",
    "4. Defined a model architecture using Convlolutional layers. Justified the model architecture based on other models, such as VGG or GoogleNet.\n",
    "5. Described how the model was trained, mentioning how the hyperparameters (optimizer, batch size, loss function, etc) were selected, and what tests / modifications to the model you performed.\n",
    "6. Tested the model on a subset of the data set and measure performance. \n",
    "7. Included images of traffic signs found from the web in a different folder, and tested the model on said images. Those images were visualized, and a discussion as to why those were chosen was included.\n",
    "8. Measured the performance on the new images and compared it to the test dataset.\n",
    "9. For the new images, output of the top 5 softmax predictions was included, and mentioned why you think those predictions were made.\n",
    "10. Achieved at least 80% accuracy on the test images.\n",
    "11. Create an additional model using transfer learning to try to improve performance.\n",
    "12. Saved the model and weights to a file for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and visualize the data\n",
    "\n",
    "Provide a basic summary of the German traffic sign data set. In the code, the analysis should be done using python, numpy, and matplotlib rather than hardcoding results manually. Include a graph of the distribution of the image classes.\n",
    "\n",
    "- The size of training set is ?\n",
    "- The size of the validation set is ?\n",
    "- The size of test set is ?\n",
    "- The shape of a traffic sign image is ?\n",
    "- The number of unique classes/labels in the data set is ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the needed code to load data set\n",
    "train_dir = pass\n",
    "test_dir = pass\n",
    "\n",
    "train_data, train_labels = pass\n",
    "test_data, test_labels = pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data set\n",
    "\n",
    "Using matplotlib, include a figure with one image from each class. Propose a class name for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the needed code to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data set\n",
    "\n",
    "Propose a preprocessing pipeline for the images. At least, the data should be normalized and augmented, but other techniques have to be considered: grayscale, blurring, using mean and standard deviation, etc. Describe what techniques you used and why. \n",
    "\n",
    "Consider using both OpenCV and [Keras' Imagedatagenerators](https://keras.io/preprocessing/image/) for this step.\n",
    "\n",
    "Additionally, include a plot of the images before and after the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design and test a model\n",
    "\n",
    "Design and implement a deep learning model, tran and test it with the german traffic sign data set. It is recommended to start from already stablished models. For reference, look at: [Common architectures in convolutional neural networks.](https://www.jeremyjordan.me/convnet-architectures/), [Convolutional networks case studies](http://cs231n.github.io/convolutional-networks/#case), [VGG Net, 1st runner of of ilsvlc 2014](https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11), [GoogLeNet, winner of ilsvrc 2014](https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7), [Resnet, winner of ilsvrc 2015](https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8).\n",
    "\n",
    "Describe the model you used and discuss why you selected it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a model based on a particular architecture. Include the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model\n",
    "\n",
    "Include a discussion on the tests / different configurations used for the optimizer, batch size, epochs, and other hyperparameters for the training. Use early stopping and save the best weights as the training develops. Include plots for the training and validation accuracy of the model. Additionally, evaluate the model with the test data. If you used a stablished architecture, mention why you think it was suitable for this problem.\n",
    "\n",
    "- training set accuracy of ?\n",
    "- validation set accuracy of ?\n",
    "- test set accuracy of ?\n",
    "\n",
    "- What were some problems with the initial architecture?\n",
    "- How was the architecture adjusted and why was it adjusted? \n",
    "- Which parameters were tuned? How were they adjusted and why?\n",
    "- Did the model over or underfit? When and why do you think it happened?\n",
    "\n",
    "After the training step, save both the model and the weights to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the training, and discuss the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on new images\n",
    "\n",
    "Find at least 10 new images from different categories and use your model to predict the traffic sign type. Plot each of the images before the prediction, after the preprocessing step, and after the prediction with its predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Load the new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the type for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use the model to make prediction on the images. Remember to preprocess the new images with \n",
    "# the same process used for the training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the performance\n",
    "\n",
    "Calculate the accuracy, and plot the images that were incorrectly classified. Discuss why those images were incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Calculate the accuracy for the new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the top 5 softmax probabilites for each image\n",
    "\n",
    "For each of the new images, print the top 5 softmax categories it predicted. Check [numpy argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) to get sorted indices of a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for each image, plot the image and print the top 5 softmax categories predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new model using transfer learning\n",
    "\n",
    "Use one of the pretrained models from [keras applications](https://keras.io/applications/#applications) to train a new model for the traffic data set. Test the model on the test data, and the new images you downloaded. As with the previous steps, record the accuracy of the model, and print the top 5 softmax predictions for the 10 images you acquired. Plot the accuracy, and the validation accuracy of this model.\n",
    "\n",
    "- Mention why you chose the selected pretrained model.\n",
    "- training set accuracy of ?\n",
    "- validation set accuracy of ?\n",
    "- test set accuracy of ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
